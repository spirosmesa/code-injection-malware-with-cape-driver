import os
import json
import shutil
import sys
import glob
import subprocess
import io
import tqdm

"""
Indicates the year to aggregate data for.
"""
YEARS = [year for year in sys.argv if year.isnumeric()]

"""
Indicates samples should be grouped together by malware family.
"""
NORMALIZE = "--normalize" in sys.argv

"""
Apply corrections on thread hijack - proces hollow
"""
CORRECT = "--correct" in sys.argv

"""
Indicates the family classification should be rebuild.
"""
FORCE_RECLASSIFY = "--reclassify" in sys.argv

class FamilyStatistics:
    def __init__(self, name: str):
        self.name = name
        self.all_samples = set()
        self.included_samples = set()
        self.positive_samples = dict()

    def add_and_include(self, sample_hash):
        self.all_samples.add(sample_hash)
        self.included_samples.add(sample_hash)

    def is_singleton(self):
        return self.name.startswith("SINGLETON:")

    def get_positive_rate(self):
        if len(self.included_samples) == 0:
            return 0
        assert set(self.positive_samples.keys()).issubset(self.included_samples)
        return len(self.positive_samples) / len(self.included_samples)

    def get_detected_behaviors(self):
        result = set()
        for sample, behaviors in self.positive_samples.items():
            result.update(behaviors)
        return result

    def get_detected_behaviors_hist(self):
        counts = dict()
        for _, behaviors in self.positive_samples.items():
            for b in behaviors:
                counts[b] = counts.get(b, 0) + 1
        return counts

    def get_preferred_behavior(self):
        if len(self.positive_samples) == 0:
            return None
        
        hist = self.get_detected_behaviors_hist()
        return sorted(hist.items(), key=lambda x: -x[1])[0][0]
        

    def has_positives(self):
        return len(self.positive_samples) > 0

    def is_all_same_behavior(self):
        if len(self.positive_samples) < len(self.included_samples):
            return len(self.positive_samples) == 0

        expected = None
        for sample, behaviors in self.positive_samples.items():
            if expected is None:
                expected = behaviors
            elif expected != behaviors:
                return False

        return True

    def __repr__(self):
        return f"{self.name}"

# Read individual sample reports and build up map from sample hash to detected behaviors.
sample_summaries = dict()
samples_per_year = {y: set() for y in YEARS}
excluded = 0

for year in YEARS:
    """
    Base directory of all reports and log files.
    """
    BASE_DIR = f"reports/{year}"

    """
    The glob for all summary files produced by the analyzer.
    """
    SUMMARY_PATH = f"{BASE_DIR}/summaries/*.log"

    """
    The directory storing all raw drakvuf logs. This is used to convert drakvuf IDs in a summary.log to sample hashes.
    """
    DRAKVUF_PATH = f"{BASE_DIR}/drakvuf-logs"

    print(SUMMARY_PATH)
    for summary_path in glob.glob(SUMMARY_PATH):
        print("Procesing", summary_path)
        for line in tqdm.tqdm(open(summary_path, "r").readlines()):
            entry = json.loads(line)
            path = entry['File']
            checksum = None

            if "Hash" in entry:
                # If hash is in the entry (old version of analyzer didn't save this), then just use it.
                checksum = entry["Hash"]
                if checksum == "<unknown>":
                    checksum = None

            if checksum is None:
                if path.endswith(".exe"):
                    # If path is an exe path, this exe's name is the hash itself.
                    checksum = path[path.rfind('/')+1:-4]
                else:
                    # Otherwise, path is a drakvuf report path, we can find the sample hash in the drakrun.log.
                    checksum = None
                    drakvuf_id = path[path.rfind('/')+1:]
                    path = f"{DRAKVUF_PATH}/{drakvuf_id}/drakrun.log"

                    if not os.path.isfile(path):
                        excluded += 1
                        sys.stderr.write(f"[!] Could not find '{path}'.\n")
                        continue

                    for x in open(path, "r").readlines():
                        idx = x.find("SHA256: ")
                        if idx != -1:
                            checksum = x[idx+8:-3]
                            break

                    if checksum is None:
                        excluded += 1
                        sys.stderr.write(f"[!] {path} does not contain sample hash.\n")
                        continue

            detected_behaviors = entry["DetectedBehaviors"]
            if CORRECT:
                if "Process Hollowing" in detected_behaviors and "Thread Hijacking" in detected_behaviors:
                    detected_behaviors.remove("Thread Hijacking")

            if checksum in sample_summaries:
                sample_summaries[checksum].update(detected_behaviors)
            else:
                sample_summaries[checksum] = set(detected_behaviors)

            samples_per_year[year].add(checksum)


for sample in sample_summaries:
    years = []
    for (year, samples) in samples_per_year.items():
        if sample in samples:
            years.append(year)
    if len(years) > 1:
        sys.stderr.write(f"[!] Sample {sample} appears in multiple years: {years}\n")


if not NORMALIZE:
    # Construct mapping from behavior to set of samples implementing said behavior.
    total_positives = 0
    behavior_to_samples = dict()
    technique_adoption_counts = dict()
    for sample_hash, behaviors in sample_summaries.items():
        if len(behaviors) > 0:
            total_positives += 1

        count = len(behaviors)
        if count > 0:
            technique_adoption_counts[count] = technique_adoption_counts.get(count, 0) + 1

        for behavior in behaviors:
            if behavior not in behavior_to_samples:
                behavior_to_samples[behavior] = set()
            behavior_to_samples[behavior].add(sample_hash)

    print("")
    print("Number of included samples:", len(sample_summaries))
    print("Number of excluded samples:", excluded)
    print(f"Positive samples: {total_positives} ({total_positives/len(sample_summaries)*100:.2f}%)")
    print("")
    print(f"{'Technique':>30}   Nr of samples   Frac of positive samples")
    for b, fs in sorted(behavior_to_samples.items(), key=lambda x: -len(x[1])):
        print(f"{b:>30}:  {len(fs):>13} {len(fs)*100.0/total_positives:>25.2f}%")

    print("")
    print(f"{'Nr of Techniques':>30}   Nr of samples   Frac of positive samples")
    for c, fs in sorted(technique_adoption_counts.items(), key=lambda x: -x[1]):
        print(f"{c:>30}:  {fs:>13} {fs*100.0/total_positives:>25.2f}%")
    sys.exit(0)


families = dict() # name -> family
hash_to_family = dict() # hash -> family
unclassified = FamilyStatistics("~unclassified~")

# Check if classification.txt exists, and if not, regenerate using avclass.
for year in YEARS:
    """
    Base directory of all reports and log files.
    """
    BASE_DIR = f"reports/{year}"
    
    """
    The path to the file containing a mapping from hash to malware family, as produced by AVClass.
    """
    CLASSIFICATION_PATH = f"{BASE_DIR}/samples/classification.txt"

    """
    The path to the directory containing metadata files of all malware samples in a set.
    """
    METADATA_PATH = f"{BASE_DIR}/samples/Win32_EXE"
    
    if not os.path.isfile(CLASSIFICATION_PATH) or os.path.getsize(CLASSIFICATION_PATH) == 0 or FORCE_RECLASSIFY:
        print(f"{CLASSIFICATION_PATH} does not exist. Regenerating from {METADATA_PATH}...")
        if not os.path.isdir(METADATA_PATH):
            sys.stderr.write(f"[!] {METADATA_PATH} does not exist. Aborting...\n")
            sys.exit(1)

        if len(os.listdir(METADATA_PATH)) == 0:
            sys.stderr.write(f"[!] {METADATA_PATH} is empty. Aborting...\n")
            sys.exit(1)

        with open(CLASSIFICATION_PATH, "w") as f, open("/tmp/classify.tmp", "w") as e:
            result = subprocess.run(
                ["python3", "avclass/avclass/avclass_labeler.py", "-vtdir", METADATA_PATH, "-hash", "sha256"],
                stdout=f,
                stderr=e
            )

        if result.returncode != 0:
            with open("/tmp/classify.tmp", "r") as e:
                sys.stderr.write(e.read())
                sys.exit(result.returncode)

    # Parse classification file.
    for line in open(CLASSIFICATION_PATH, "r").readlines():
        [sample_hash, family] = line.split('\t')
        family_name = family.strip()
        family = families.get(family_name)
        
        if family is None:
            # Create a new family if it doesn't exist yet.
            family = FamilyStatistics(family_name)
            families[family_name] = family

            # Collect all singleton families, since we want to group these together in the end 
            # in the family distribution overview.
            if family.is_singleton():
                unclassified.all_samples.add(sample_hash)

        family.all_samples.add(sample_hash)
        hash_to_family[sample_hash] = family


# Find out which samples were included in each family.
not_found = 0
for sample_hash, behaviors in sample_summaries.items():
    family = hash_to_family.get(sample_hash)
    if family is None:
        # This is a family that wasn't specified in the classification.txt, assume it's a singleton family.
        not_found += 1
        family = FamilyStatistics("SINGLETON:" + sample_hash)
        family.add_and_include(sample_hash)
        unclassified.add_and_include(sample_hash)
        continue

    family.add_and_include(sample_hash)
    if len(behaviors) > 0:
        x = family.positive_samples.get(sample_hash)
        if x is None:
            x = set()
            family.positive_samples[sample_hash] = x
        x.update(behaviors)

# Filter out families with no included samples.
families = { n: f for (n, f) in families.items() if len(f.included_samples) > 0 }

# Construct mapping from behavior to set of families implementing said behavior.
behavior_to_families = dict()
counts_at_least_one = 0
counts_all_same = 0
for _, family in families.items():
    if family.has_positives():
        counts_at_least_one += 1
    if family.is_all_same_behavior():
        counts_all_same += 1

    for behavior in family.get_detected_behaviors():
        if behavior not in behavior_to_families:
            behavior_to_families[behavior] = set()
        behavior_to_families[behavior].add(family)

print("")
print("Number of included samples:", len(sample_summaries))
print("Number of excluded samples:", excluded)
print("Number of families:", len(families))
print("Number of samples missing in classification:", not_found)
print(f"All same behavior: {counts_all_same} ({counts_all_same*100.0/len(families):.2f}%)")
print(f"At least one injection: {counts_at_least_one} ({counts_at_least_one*100.0/len(families):.2f}%)")
print("")

print("Family Distribution:")
print("")
#
# top_family_count = 18
# top_families     = dict(sorted(families.items(), key=lambda x: -len(x[1].included_samples))[:top_family_count])
# top_family_names = sorted(families.keys(), key=lambda x: -len(families[x].included_samples))[:top_family_count]
top_family_names = [
          "virlock",
           "dinwod",
            "sivis",
           "berbew",
           "upatre",
            "virut",
             "delf",
           "kolabc",
           "vobfus",
           "wapomi",
            "wabot",
           "vindor",
          "allaple",
            "gator",
         "hematite",
        "vtflooder",
           "shipup",
            "gepys",
]
top_families = {n:f for n,f in families.items() if n in top_family_names}

other = FamilyStatistics("Other")
for name, family in families.items():
    if name in top_families:
        continue
    other.included_samples.update(family.included_samples)
    other.positive_samples.update(family.positive_samples)

other.included_samples.update(unclassified.included_samples)
other.positive_samples.update(unclassified.positive_samples)

def print_family_stats(family):
    short_names = {
        "Process Hollowing":              "Hollow",
        "Thread Hijacking":               "Thread",
        "CTray VTable":                   "CTray",
        "APC Shell Injection":            "APCShl",
        "APC DLL Injection":              "APCDll",
        "Generic Shell Injection":        "Shell",
        "Classic DLL Injection":          "DLL",
        "Shim Injection":                 "Shim",
        "Image File Execution Options":   "IFEO",
        "AppInit DLL Injection":          "AppInit",
        "AppCertDlls Injection":          "AppCert",
        "COM Hijack DLL Injection":       "COM",
        "SetWindowsHookEx DLL Injection": "WinHook"
    }

    print(f"{family.name:>17} &  {len(family.included_samples):>5} &  {family.get_positive_rate()*100:>6.2f}\\% & {short_names.get(family.get_preferred_behavior()) or family.get_preferred_behavior() or ' ':>7} &")


#for _, family in sorted(top_families.items(), key=lambda x: -len(x[1].included_samples)):
for name in top_family_names:
    print_family_stats(top_families.get(name) or FamilyStatistics(name))
print_family_stats(other)

print("")
print(f"{'Technique':>30}   Nr of Families   Frac of positive families")
for b, fs in sorted(behavior_to_families.items(), key=lambda x: -len(x[1])):
    sorted_fs = sorted(fs, key=lambda x: -len(x.included_samples))
    s = ", ".join(f.name for f in sorted_fs if not f.is_singleton())
    print(f"{b:>30}:  {len(fs):>14} {len(fs)*100.0/counts_at_least_one:>26.2f}%")

print("")
